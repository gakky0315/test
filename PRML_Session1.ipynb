{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f65107",
   "metadata": {},
   "source": [
    "# セッション1: モデル選択と情報理論\n",
    "概要:   \n",
    "本セッションでは、モデルの複雑さと過学習、次元の呪い、基本的な情報理論の概念をPythonで体験します。具体的には、ポリノミナル回帰モデルでの過学習例、データの次元が増えることによる最近傍距離の変化、離散分布間のKLダイバージェンス計算などを行います。\n",
    "\n",
    "## 目的\n",
    "- 過学習と汎化誤差の違いを理解する\n",
    "- 次元の呪いの影響を可視化する\n",
    "- 情報理論の基礎（KLダイバージェンス）を実装で確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685b7b0",
   "metadata": {},
   "source": [
    "## 問題2: 次元の呪い\n",
    "高次元空間における「最近傍点と最遠点の距離比」を計算し、次元が上がるとどうなるかを確認しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ace7e-1d9d-487a-a9e5-ba714c798bb2",
   "metadata": {},
   "source": [
    "高次元データ空間における直感的でない現象を確認します。特に、ユニットハイパーキューブ（各次元が[0,1]区間）内で点をランダムに生成し、あるクエリ点からの最近傍距離と最遠距離の比や差を次元数に応じて調べます。次元が増加すると、最近傍点と最遠点までの距離にどのような変化が起きるでしょうか。これが「次元の呪い」と呼ばれる現象の一端です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45855b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [1, 2, 5, 10, 50, 100]\n",
    "num_points = 1000\n",
    "\n",
    "for d in dims:\n",
    "    # [0,1]^d 内に一様に num_points 個の乱数点を生成\n",
    "    data = np.random.rand(num_points, d)\n",
    "    # クエリ点もランダムに1つ生成\n",
    "    query = np.random.rand(1, d)\n",
    "    # ユークリッド距離を計算\n",
    "    distances = np.linalg.norm(data - query, axis=1)\n",
    "    nearest = distances.min()\n",
    "    farthest = distances.max()\n",
    "    ratio = ____ / ____\n",
    "    print(f\"{d}次元: 最近傍距離={nearest:.4f}, 最遠距離={farthest:.4f}, 比率={ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a46356-9433-4fb0-b5df-c4540f8759d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1次元: 最近傍距離=0.0006, 最遠距離=0.5923, 比率(nearest/farthest)=0.0011\n",
      "2次元: 最近傍距離=0.0143, 最遠距離=0.9671, 比率(nearest/farthest)=0.0148\n",
      "5次元: 最近傍距離=0.2083, 最遠距離=1.5257, 比率(nearest/farthest)=0.1365\n",
      "10次元: 最近傍距離=0.4382, 最遠距離=1.8743, 比率(nearest/farthest)=0.2338\n",
      "50次元: 最近傍距離=2.0946, 最遠距離=3.3080, 比率(nearest/farthest)=0.6332\n",
      "100次元: 最近傍距離=3.4237, 最遠距離=4.7586, 比率(nearest/farthest)=0.7195\n"
     ]
    }
   ],
   "source": [
    "# 解答\n",
    "import numpy as np\n",
    "\n",
    "# パラメータ設定\n",
    "num_points = 1000    # 各次元で生成する点の数\n",
    "dims = [1, 2, 5, 10, 50, 100]  # 調べるデータの次元次\n",
    "np.random.seed(0)\n",
    "\n",
    "for d in dims:\n",
    "    # [0,1]^d 内に一様に num_points 個の乱数点を生成\n",
    "    data = np.random.rand(num_points, d)\n",
    "    # クエリ点もランダムに1つ生成\n",
    "    query = np.random.rand(1, d)\n",
    "    # ユークリッド距離を計算\n",
    "    distances = np.linalg.norm(data - query, axis=1)\n",
    "    nearest = distances.min()\n",
    "    farthest = distances.max()\n",
    "    ratio = nearest / farthest\n",
    "    print(f\"{d}次元: 最近傍距離={nearest:.4f}, 最遠距離={farthest:.4f}, 比率(nearest/farthest)={ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53b794-9caf-4a7e-a4c2-f09b7d42441d",
   "metadata": {},
   "source": [
    "このように、次元が高くなるにつれて最近傍距離と最遠距離の差が縮まり、比率が1に近づいていることが分かります。1次元ではクエリに非常に近い点と遠い点の距離差が大きいですが、100次元にもなると最も近い点でさえクエリ点からかなり離れており、最遠点との距離との差は相対的に小さくなっています。**「高次元では全ての点がほぼ同じ距離にある」**という現象の一例です。これは高次元空間ではデータがスカスカになる（体積が指数的に増加する）ためであり、距離に基づく手法（例: k近傍法）の識別力が低下する原因となります。この結果は「次元の呪い」の典型例であり、高次元データを扱う際には次元削減や適切な距離尺度の選択が重要になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bf0db",
   "metadata": {},
   "source": [
    "## 問題3: KLダイバージェンス\n",
    "2つの離散確率分布間のKLダイバージェンスを計算してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7734d-1e15-4151-91be-bd12c0171c0e",
   "metadata": {},
   "source": [
    "情報理論の観点から、確率分布間の距離（非対称な指標）であるKLダイバージェンスを計算し解釈します。2つの簡単な確率分布間のKLダイバージェンスをPythonで計算し、その非対称性と値の意味を確認してください。また、エントロピーやクロスエントロピーとの関係についても考察します（必要に応じて）。ここでは例として、コインの表裏の分布を扱います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea28c37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m KL_QP \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(Q \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(Q \u001b[38;5;241m/\u001b[39m P))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# TODO: 結果を出力して非対称性を確認しましょう\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL(P||Q) =\u001b[39m\u001b[38;5;124m\"\u001b[39m, ____)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL(Q||P) =\u001b[39m\u001b[38;5;124m\"\u001b[39m, ____)\n",
      "\u001b[1;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "# 2つの離散分布PとQを定義（例: コインの表裏の確率分布）\n",
    "P = np.array([0.8, 0.2])  # P: 表が80%, 裏が20%のコイン\n",
    "Q = np.array([0.5, 0.5])  # Q: 表裏50%ずつの公平なコイン\n",
    "\n",
    "# KLダイバージェンス D(P||Q) の計算\n",
    "# 定義: D(P||Q) = Σ P(x) * log( P(x) / Q(x) )\n",
    "KL_PQ = np.sum(P * np.log(P / Q))\n",
    "\n",
    "# KLダイバージェンス D(Q||P) も計算\n",
    "KL_QP = np.sum(Q * np.log(Q / P))\n",
    "\n",
    "# TODO: 結果を出力して非対称性を確認しましょう\n",
    "print(\"KL(P||Q) =\", ____)\n",
    "print(\"KL(Q||P) =\", ____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032c098c-6696-439f-b651-36acb5a4a911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P||Q) = 0.1927\n",
      "KL(Q||P) = 0.2231\n"
     ]
    }
   ],
   "source": [
    "# 解答\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 2つの離散分布PとQを定義（例: コインの表裏の確率分布）\n",
    "P = np.array([0.8, 0.2])  # P: 表が80%, 裏が20%のコイン\n",
    "Q = np.array([0.5, 0.5])  # Q: 表裏50%ずつの公平なコイン\n",
    "\n",
    "# KLダイバージェンス D(P||Q) の計算\n",
    "# 定義: D(P||Q) = Σ P(x) * log( P(x) / Q(x) )\n",
    "KL_PQ = np.sum(P * np.log(P / Q))\n",
    "\n",
    "# KLダイバージェンス D(Q||P) も計算\n",
    "KL_QP = np.sum(Q * np.log(Q / P))\n",
    "\n",
    "print(f\"KL(P||Q) = {KL_PQ:.4f}\")\n",
    "print(f\"KL(Q||P) = {KL_QP:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df71eb9-8ecb-482d-8a35-b3378df9857d",
   "metadata": {},
   "source": [
    "となります。重要な点は、KL(P||Q) と KL(Q||P) は一般に等しくならない（非対称性）ことです。この例でも値が異なり、\n",
    "$D(P||Q) = 0.2579$ は、真の分布Pを仮定してそれをQで近似したときの情報損失を意味します。Pでは表が出る確率が高いのにQでは50%と仮定しているため、それなりの情報ロスが発生しています。\n",
    "$D(Q||P) = 0.2231$ は、逆に公平なコインQを偏ったコインPでモデル化した場合の情報損失であり、こちらは若干小さい値です。\n",
    "KLダイバージェンスの単位はナット（自然対数の場合）で、この値が0であれば2つの分布は同一、値が大きいほど分布間の差異が大きいことを表します。常に0以上であること（非負性）も確認できます。この指標は情報理論だけでなく、機械学習の最適化（例: $⾮\\mathrm{負}\\mathrm{log}$尤度はデータ分布とモデル分布のKLダイバージェンスに対応）や、モデル間距離の評価など様々な場面で現れます。なお、エントロピーH(P)とクロスエントロピーH(P, Q)を用いれば $D(P||Q) = H(P, Q) - H(P)$ と表せます。この例で計算すれば、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bad4af5-c7d8-4258-8222-034f655250ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19274475702175742\n"
     ]
    }
   ],
   "source": [
    "H_P = -np.sum(P * np.log(P))            # Pのエントロピー\n",
    "H_PQ = -np.sum(P * np.log(Q))           # Pに対するQのクロスエントロピー\n",
    "print(H_PQ - H_P)  # KLと一致するはず\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f262562-a336-4064-9665-dfbe865b7274",
   "metadata": {},
   "source": [
    "とすることでKLの値と一致することが確かめられます。これらの概念は情報理論(1.6節)で詳しく述べられています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17048af-1f41-44d2-942a-93f1c71f9802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
