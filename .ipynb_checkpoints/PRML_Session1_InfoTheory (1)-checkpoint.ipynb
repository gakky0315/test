{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0390ecf7",
   "metadata": {},
   "source": [
    "# セッション1: 情報理論の基礎と応用\n",
    "\n",
    "このノートブックでは、情報理論の主要な概念である**情報量**, **エントロピー**, **KLダイバージェンス**について数式とPythonコードを通して直感的に理解を深めます。\n",
    "\n",
    "## 目標\n",
    "- 情報量が確率とどのような関係にあるかを理解する\n",
    "- 離散エントロピーとその最大・最小の意味を確認する\n",
    "- KLダイバージェンスがどのような「距離」なのかを実感する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb74be",
   "metadata": {},
   "source": [
    "## 問題1: 情報量の性質と可視化\n",
    "\n",
    "情報量 \\( I(x) = - \\log_2 p(x) \\) は、事象の起こりにくさを定量化した指標です。  \n",
    "以下では、確率 \\( p \\in (0, 1] \\) に対して情報量をグラフ化し、低い確率が高い情報量に対応することを確認しましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p = np.linspace(0.001, 1, 100)\n",
    "info = -np.log2(p)\n",
    "\n",
    "plt.plot(p, info)\n",
    "plt.xlabel(\"確率 p\")\n",
    "plt.ylabel(\"情報量 I(x) = -log2(p)\")\n",
    "plt.title(\"情報量と確率の関係\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 情報量が最大となるのは、p = ____ のとき\n",
    "# 情報量が最小となるのは、p = ____ のとき\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"最大情報量の確率 p =\", p[np.argmax(info)])\n",
    "print(\"最小情報量の確率 p =\", p[np.argmin(info)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb569e4",
   "metadata": {},
   "source": [
    "## 問題2: エントロピー最大化の直感\n",
    "\n",
    "エントロピー \\( H(p) = -\\sum p(x) \\log_2 p(x) \\) は「平均的な情報量」を意味します。  \n",
    "等確率（最も不確実）な分布と、偏った（確定的）分布を比較し、エントロピーがどう変化するかを確認しましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f518678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return -np.sum(p * np.log2(p + 1e-12))\n",
    "\n",
    "# 等確率な分布（最大エントロピー）\n",
    "p_uniform = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "# 偏った分布（最小エントロピーに近い）\n",
    "p_biased = np.array([0.97, 0.01, 0.01, 0.01])\n",
    "\n",
    "print(\"等確率分布のエントロピー =\", entropy(p_uniform))\n",
    "print(\"偏った分布のエントロピー =\", entropy(p_biased))\n",
    "\n",
    "# TODO: 等確率分布の方がエントロピーが ____ くなる理由を考察せよ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec09a6",
   "metadata": {},
   "source": [
    "## 問題3: KLダイバージェンスの直感\n",
    "\n",
    "KLダイバージェンスは、真の分布 \\( p \\) と予測分布 \\( q \\) の「ずれ」を測る非対称な指標です：\n",
    "\n",
    "\\[\n",
    "\\mathrm{KL}(p || q) = \\sum p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right)\n",
    "\\]\n",
    "\n",
    "以下のコードを完成させて、\\( KL(p||q) \\) を実装し、2つのケースを比較してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fe98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return np.sum(p * np.log2(p / q))\n",
    "\n",
    "p = np.array([0.4, 0.6])\n",
    "q1 = np.array([0.5, 0.5])  # 少しずれた予測\n",
    "q2 = np.array([0.9, 0.1])  # 大きくずれた予測\n",
    "\n",
    "kl1 = kl_divergence(p, q1)\n",
    "kl2 = kl_divergence(p, q2)\n",
    "\n",
    "print(\"KL(p || q1) =\", kl1)\n",
    "print(\"KL(p || q2) =\", kl2)\n",
    "\n",
    "# q1 と q2 のどちらが p に近い？そのとき KL ダイバージェンスは ____ くなる。\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
