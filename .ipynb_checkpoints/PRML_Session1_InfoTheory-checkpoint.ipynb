{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6833fece",
   "metadata": {},
   "source": [
    "# セッション1: 情報理論の基礎と応用\n",
    "\n",
    "本ノートブックでは、情報理論の核心であるエントロピー・KLダイバージェンス・情報量の直感を、Pythonを用いて実験的に理解します。\n",
    "\n",
    "## 学習目標\n",
    "- 情報量とは何かを直感的・定量的に理解する\n",
    "- エントロピーの定義と意味を確認する\n",
    "- 確率分布間の距離としてのKLダイバージェンスを実装する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67872f37",
   "metadata": {},
   "source": [
    "## 問題1: 情報量と確率\n",
    "\n",
    "ある事象の情報量は、その確率が低いほど大きくなることが知られています。以下のセルで、いくつかの確率に対する情報量（驚きの度合い）を計算してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3652f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 対象の確率\n",
    "probabilities = [0.5, 0.25, 0.1, 0.01]\n",
    "\n",
    "# 情報量（self-information） I(x) = -log2(p(x)) を計算\n",
    "for p in probabilities:\n",
    "    info = -np.log2(p)\n",
    "    print(f\"p = {p:.2f} -> 情報量 I(x) = {'____'}\")  # <- 穴埋め\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de01dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in probabilities:\n",
    "    info = -np.log2(p)\n",
    "    print(f\"p = {p:.2f} -> 情報量 I(x) = {info:.4f} bit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18c2a5",
   "metadata": {},
   "source": [
    "## 問題2: エントロピー計算\n",
    "\n",
    "確率分布 \\( p = [0.25, 0.25, 0.25, 0.25] \\) における情報の期待値、すなわちエントロピーを計算しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "entropy = -np.sum(p * np.log2(p))\n",
    "print(\"エントロピー H(p) =\", '____')  # <- 穴埋め\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d931f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"エントロピー H(p) =\", entropy)  # 解答\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a67ab",
   "metadata": {},
   "source": [
    "## 問題3: KLダイバージェンス\n",
    "\n",
    "真の分布 \\( p = [0.5, 0.5] \\)、予測分布 \\( q = [0.9, 0.1] \\) に対するKLダイバージェンスを計算してみましょう。\n",
    "\n",
    "\\[\n",
    "\\mathrm{KL}(p || q) = \\sum p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right)\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([0.5, 0.5])\n",
    "q = np.array([0.9, 0.1])\n",
    "\n",
    "kl = np.sum(p * np.log2(p / q))\n",
    "print(\"KL(p || q) =\", '____')  # <- 穴埋め\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KL(p || q) =\", kl)  # 解答\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
